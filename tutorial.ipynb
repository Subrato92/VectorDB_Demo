{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone \n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github = https://github.com/krishnaik06/Complete-Langchain-Tutorials/tree/main/LLM%20Generic%20APP\n",
    "# youtube = https://www.youtube.com/watch?v=erUfLIi9OFM&t=1809s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Read the document\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_doc('docs/') ## Page wise docs\n",
    "print('Loaded Docs: ', len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks\n",
    "### https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html#\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=50):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=chunk_data(docs=doc)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY=os.environ['MISTRAL_API_KEY']\n",
    "print(MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding Technique Of OPENAI\n",
    "\"\"\"\n",
    "api_key=os.environ['OPENAI_API_KEY']\n",
    "print(api_key)\n",
    "\n",
    "pinecone_api_key=os.environ['PINECONE_API_KEY']\n",
    "print(pinecone_api_key)\n",
    "\n",
    "embeddings=OpenAIEmbeddings(api_key=api_key)\n",
    "embeddings\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import getpass\n",
    "\n",
    "# if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "#    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "import time\n",
    "\n",
    "index_name = \"langchain-test-index\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "index.list\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "llm=OpenAI(model_name=\"text-davinci-003\",temperature=0.5)\n",
    "chain=load_qa_chain(llm,chain_type=\"stuff\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "mistral_model = \"mistral-embed\"\n",
    "\n",
    "mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "embeddings_batch_response = mistral_client.embeddings.create(\n",
    "    model=mistral_model,\n",
    "    inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n",
    ")\n",
    "\n",
    "print(embeddings_batch_response)\n",
    "TOKEN_LENGTH = len(embeddings_batch_response.data[0].embedding)\n",
    "print('Token Len:', TOKEN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "st_model = SentenceTransformer(\"nvidia/NV-Embed-v2\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\"\n",
    "]\n",
    "embeddings = st_model.encode(sentences)\n",
    "\n",
    "print(embeddings)\n",
    "# [3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"llama3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embed.embed_query(\"How are you?\")\n",
    "print(\"Vactor len:\", len(vector))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[2:4]\n",
    "print(\"Len:\", len(documents), type(documents[0]))\n",
    "doc_subset = documents[:10]\n",
    "print(\"Len doc_subset:\", len(doc_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "  { \n",
    "    \"id\":uuids[i], \n",
    "    \"metadata\": documents[i].metadata, \n",
    "    #\"vector\": embed.embed_query(documents[i].page_content),\n",
    "    \"vector\": mistral_client.embeddings.create( model=mistral_model, inputs=[documents[i].page_content],).data[0].embedding,\n",
    "    \"content\": documents[i].page_content \n",
    "  } for i in range(len(documents))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(data[0], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(json.dumps(data[0], indent=4, sort_keys=True))\n",
    "print(len(data[0]['vector']))\n",
    "print(len(data[0]['id']))\n",
    "max_len = 0\n",
    "for doc in data:\n",
    "    if len(doc[\"content\"]) > max_len:\n",
    "        max_len = len(doc[\"content\"])\n",
    "        \n",
    "print('max_len:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\n",
    "    uri=\"http://localhost:19530\",\n",
    "    token=\"root:Milvus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "client.create_database(\n",
    "    db_name=\"my_database_1\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import DataType\n",
    "\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "# 2.2. Add fields to schema\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, max_length=36, is_primary=True)\n",
    "schema.add_field(field_name=\"content\", datatype=DataType.VARCHAR, max_length=(max_len+200))\n",
    "schema.add_field(field_name=\"metadata\", datatype=DataType.JSON)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=TOKEN_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    index_name=\"vector_index\",\n",
    "    index_type=\"FLAT\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"demo_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=COLLECTION_NAME):\n",
    "    client.drop_collection(collection_name=COLLECTION_NAME)\n",
    "    \n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    dimension=4096,  # The vectors we will use in this demo has 768 dimensions\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.insert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "llmModel = OllamaLLM(model=\"llama3\")\n",
    "chain=load_qa_chain(llmModel, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity Retreive Results from VectorDB\n",
    "def retrieve_query(query,k=2):\n",
    "    query_vector = embed.embed_query(query)\n",
    "    print(query_vector)\n",
    "    matching_results = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        anns_field=\"vector\",\n",
    "        data=[query_vector],\n",
    "        limit=k,\n",
    "        search_params={\"metric_type\": \"COSINE\"},\n",
    "        output_fields=[\"content\", \"metadata\"]\n",
    "    )\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answers(query):\n",
    "    doc_search=retrieve_query(query, 4)\n",
    "    from langchain_core.documents import Document\n",
    "\n",
    "    documents = []\n",
    "    for hits in doc_search:\n",
    "        for hit in hits:\n",
    "            documents.append(Document(page_content=hit[\"entity\"][\"content\"], metadata=hit[\"entity\"][\"metadata\"]))\n",
    "            #print(json.dumps(hit, indent=4, sort_keys=True))\n",
    "    \n",
    "    print(json.dumps(doc_search[0], indent=4, sort_keys=True))\n",
    "            \n",
    "    ip = {\n",
    "        \"input_documents\":documents,\n",
    "        \"question\": query\n",
    "    }\n",
    "    response=chain.invoke(input=ip)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.load_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    load_fields=[\"vector\", \"id\", \"content\", \"metadata\"] # Load only the specified fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_query = \"How much the agriculture target will be increased by how many crore?\"\n",
    "answer = retrieve_answers(our_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"answer: \", type(answer), list(answer.keys()), )\n",
    "print(answer[\"output_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_demo_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
